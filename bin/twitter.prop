#location of the training file
trainFile = ./src/CoNLL_final.tsv
#location where you would like to save (serialize to) your
#classifier; adding .gz at the end automatically gzips the file,
#making it faster and smaller
serializeTo = 10-twitter-ner-model.ser.gz

#structure of your training file; this tells the classifier
#that the word is in column 0 and the correct answer is in
#column 1
map = word=0,answer=1,lemma=2,tag=3

#these are the features we'd like to train with
#some are discussed below, the rest can be
#understood by looking at NERFeatureFactory
useClassFeature=true
useWord=true
useNGrams=true
#no ngrams will be included that do not contain either the
#beginning or end of the word
noMidNGrams=true
useDisjunctive=true
#maxNGramLeng=6
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
#the next 4 deal with word shape features
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
#wordShape=chris2useLC

# dan2 better than chris2 on CoNLL data...
wordShape=dan2useLC
useDisjunctive=true
# disjunctionWidth 4 is better than 5 on CoNLL data
disjunctionWidth=4
sigma = 20
useQN = true
QNsize = 25
useLastRealWord=true
useNextRealWord=true


#memory parameters
cacheNGrams=true
maxLeft=1
#qnSize=10
saveFeatureIndexToDisk=true
#useObservedSequencesOnly=true

#improving quality
useLemmas=true
usePrevNextLemmas=true
useTags=true
useGazettes=true
gazette=./src/B-PER.txt ./src/companies.txt ./src/location.txt ./src/misc.txt
cleanGazette=true

useOccurrencePatterns=true
useTitle2=true